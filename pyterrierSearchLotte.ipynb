{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c78a794d-091d-45f1-b453-2a756d62f551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim.downloader as gensim_downloader\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert import Searcher\n",
    "import joblib\n",
    "import warnings\n",
    "import os\n",
    "import datetime\n",
    "import ir_datasets\n",
    "import pyterrier as pt\n",
    "from pyterrier.measures import *\n",
    "\n",
    "dataset =  pt.get_dataset(\"irds:lotte/lifestyle/dev/search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bef895f2-e4ec-49e1-8da0-215dbfc699e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lotte/lifestyle/dev/search documents: 100%|███████████████████████████████████████████████████████████████████████████████████| 268893/268893 [03:16<00:00, 1366.08it/s]\n"
     ]
    }
   ],
   "source": [
    "index_path = './lotteindex/'\n",
    "index = pt.index.IterDictIndexer(\n",
    "    index_path,\n",
    "    type=pt.index.IndexingType.MEMORY,\n",
    ").index(dataset.get_corpus_iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bab0339e-2235-4fc2-b961-d3949ef3ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word2vec_model = gensim_downloader.load(\"glove-wiki-gigaword-100\")\n",
    "vectorizer = joblib.load('trained/vectorizerlotte.pkl')\n",
    "file = open(\"trained/doc_vecslotte.pickle\",'rb') \n",
    "doc_vecs = pickle.load(file)\n",
    "data_dir = './project-root/lotte/raw/'\n",
    "collection = pd.read_csv(data_dir + \"collection.tsv\", sep='\\t', \n",
    "                                names=['doc_id', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10ba58ea-fa41-4b4e-8ebc-9ee3a08bc385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1250/3173451093.py:4: DeprecationWarning: Call to deprecated function (or staticmethod) autoclass. (use pt.java.autoclass(...) instead) -- Deprecated since version 0.11.0.\n",
      "  tokenizer = pt.autoclass(\"org.terrier.indexing.tokenisation.Tokeniser\").getTokeniser()\n"
     ]
    }
   ],
   "source": [
    "if not pt.java.started():\n",
    "    pt.init()\n",
    "\n",
    "tokenizer = pt.autoclass(\"org.terrier.indexing.tokenisation.Tokeniser\").getTokeniser()\n",
    "def strip_markup(text):\n",
    "    return \" \".join(tokenizer.getTokens(text))\n",
    "\n",
    "def _preprocess_text(text: str) -> list:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "\n",
    "def stop_lemma(text: str) -> list:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return ' '.join([lemmatizer.lemmatize(token) for token in tokens if token not in stop_words])\n",
    "\n",
    "def stop_porter(text: str) -> list:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return ' '.join([stemmer.stem(token) for token in tokens if token not in stop_words])\n",
    "    \n",
    "def stop_word(text: str) -> list:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return ' '.join([token for token in tokens if token not in stop_words])\n",
    "\n",
    "def expand_query_wordnet(query: str, num_expansions: int = 2) -> str:\n",
    "    tokens = _preprocess_text(query)\n",
    "    expanded_terms = set(tokens)\n",
    "    for token in tokens:\n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(token)[:2]:  # Limit to top 2 synsets\n",
    "            for lemma in syn.lemmas()[:num_expansions]:\n",
    "                synonym = lemma.name().lower()\n",
    "                if synonym != token and synonym not in synonyms:\n",
    "                    synonyms.add(synonym)\n",
    "            if len(synonyms) >= num_expansions:\n",
    "                break\n",
    "        expanded_terms.update(synonyms)\n",
    "    return ' '.join(expanded_terms)\n",
    "\n",
    "def expand_query_word2vec(query: str, num_expansions: int = 2, threshold: float = 0.7) -> str:\n",
    "    topn=3\n",
    "    words = query.split()\n",
    "    expanded_words = words.copy()\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            similar_words = [w for w, _ in word2vec_model.most_similar(word, topn=topn) \n",
    "                            if w.lower() != word.lower()]\n",
    "            expanded_words.extend(similar_words)\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return ' '.join(expanded_words)\n",
    "\n",
    "def expand_query_pseudo_relevance(doc_vecs, query: str, collection: pd.DataFrame, \n",
    "                                 vectorizer: TfidfVectorizer, top_k: int = 3, \n",
    "                                 num_expansions: int = 2) -> str:\n",
    "    try:\n",
    "        query_vec = vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_vec, doc_vecs)[0]\n",
    "        top_k_indices = np.argsort(similarities)[-top_k:]\n",
    "        top_k_docs = collection.iloc[top_k_indices]\n",
    "        top_k_vecs = vectorizer.transform(top_k_docs['text']).toarray()\n",
    "        mean_top_k = np.mean(top_k_vecs, axis=0)\n",
    "        original_vec = query_vec.toarray()[0]\n",
    "        combined_vec = 0.7 * mean_top_k + 0.3 * original_vec  # Rocchio-like weighting\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        top_indices = np.argsort(combined_vec)[-num_expansions:]\n",
    "        expansion_terms = [feature_names[idx] for idx in top_indices \n",
    "                          if feature_names[idx] not in query.lower().split()]\n",
    "    except IndexError:\n",
    "        print(len(collection))\n",
    "        print(collection)\n",
    "        print(vectorizer)\n",
    "        print(query)\n",
    "        print(doc_vecs)\n",
    "        print(query_vec)\n",
    "        print(top_k_indices)\n",
    "    return query + ' ' + ' '.join(expansion_terms)\n",
    "\n",
    "def expand_porter_stemmer(text: str) -> str:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return ' '.join([stemmer.stem(token) for token in tokens])\n",
    "    \n",
    "def comb(text: str, doc_vecs, collection, vectorizer) -> str:\n",
    "    q = expand_query_wordnet(text)\n",
    "    q = expand_query_word2vec(q)\n",
    "    q = expand_query_pseudo_relevance(doc_vecs, q, collection, vectorizer)\n",
    "    return expand_porter_stemmer(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87878697-3a8f-4483-9ca1-66c79e53778b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>AP</th>\n",
       "      <th>RR</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>AP +</th>\n",
       "      <th>AP -</th>\n",
       "      <th>AP p-value</th>\n",
       "      <th>RR +</th>\n",
       "      <th>RR -</th>\n",
       "      <th>RR p-value</th>\n",
       "      <th>nDCG@10 +</th>\n",
       "      <th>nDCG@10 -</th>\n",
       "      <th>nDCG@10 p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>0.402428</td>\n",
       "      <td>0.542495</td>\n",
       "      <td>0.466339</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stop</td>\n",
       "      <td>0.402499</td>\n",
       "      <td>0.541729</td>\n",
       "      <td>0.466227</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.462129e-02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.197485e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stop-Porter</td>\n",
       "      <td>0.389080</td>\n",
       "      <td>0.519141</td>\n",
       "      <td>0.448789</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.937568e-02</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.106647</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.909043e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop-Lemma</td>\n",
       "      <td>0.402438</td>\n",
       "      <td>0.541606</td>\n",
       "      <td>0.466227</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.674949e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.292129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.197485e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stemming</td>\n",
       "      <td>0.388123</td>\n",
       "      <td>0.519572</td>\n",
       "      <td>0.448334</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.565601e-02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.112845</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.228793e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wordnet</td>\n",
       "      <td>0.364904</td>\n",
       "      <td>0.524304</td>\n",
       "      <td>0.419810</td>\n",
       "      <td>20.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.568160e-02</td>\n",
       "      <td>19.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.520091</td>\n",
       "      <td>13.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>6.128246e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0.279027</td>\n",
       "      <td>0.388899</td>\n",
       "      <td>0.319963</td>\n",
       "      <td>14.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8.067188e-07</td>\n",
       "      <td>13.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>14.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.976985e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pseudo-relevance</td>\n",
       "      <td>0.378381</td>\n",
       "      <td>0.515820</td>\n",
       "      <td>0.433032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>6.793019e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.066340e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Combined</td>\n",
       "      <td>0.273699</td>\n",
       "      <td>0.397468</td>\n",
       "      <td>0.319803</td>\n",
       "      <td>13.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>6.343878e-06</td>\n",
       "      <td>13.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>13.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.650005e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name        AP        RR   nDCG@10  AP +  AP -    AP p-value  \\\n",
       "0              None  0.402428  0.542495  0.466339   NaN   NaN           NaN   \n",
       "1              Stop  0.402499  0.541729  0.466227   5.0   2.0  4.462129e-02   \n",
       "2       Stop-Porter  0.389080  0.519141  0.448789   7.0  11.0  4.937568e-02   \n",
       "3        Stop-Lemma  0.402438  0.541606  0.466227   4.0   3.0  8.674949e-01   \n",
       "4          Stemming  0.388123  0.519572  0.448334   2.0  14.0  3.565601e-02   \n",
       "5           Wordnet  0.364904  0.524304  0.419810  20.0  55.0  1.568160e-02   \n",
       "6          Word2Vec  0.279027  0.388899  0.319963  14.0  73.0  8.067188e-07   \n",
       "7  Pseudo-relevance  0.378381  0.515820  0.433032   0.0  69.0  6.793019e-07   \n",
       "8          Combined  0.273699  0.397468  0.319803  13.0  74.0  6.343878e-06   \n",
       "\n",
       "   RR +  RR -  RR p-value  nDCG@10 +  nDCG@10 -  nDCG@10 p-value  \n",
       "0   NaN   NaN         NaN        NaN        NaN              NaN  \n",
       "1   2.0   1.0    0.361568        0.0        1.0     3.197485e-01  \n",
       "2   4.0   9.0    0.106647        2.0        9.0     6.909043e-02  \n",
       "3   1.0   2.0    0.292129        0.0        1.0     3.197485e-01  \n",
       "4   2.0  10.0    0.112845        2.0       10.0     6.228793e-02  \n",
       "5  19.0  28.0    0.520091       13.0       36.0     6.128246e-03  \n",
       "6  13.0  60.0    0.000014       14.0       56.0     6.976985e-08  \n",
       "7   0.0  30.0    0.000580        0.0       27.0     6.066340e-05  \n",
       "8  13.0  58.0    0.000285       13.0       56.0     1.650005e-06  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  \n",
    "no_qe = pt.terrier.Retriever(index, wmodel=\"BM25\", metadata=[\"docno\", \"text\"], properties={\"termpipelines\": \"\"}, controls={\"qe\": \"off\"})\n",
    "qe_stop = pt.apply.query(lambda q: strip_markup(stop_word(q[\"query\"]))) >> no_qe\n",
    "qe_sp = pt.apply.query(lambda q: strip_markup(stop_porter(q[\"query\"]))) >> no_qe\n",
    "qe_sl = pt.apply.query(lambda q: strip_markup(stop_lemma(q[\"query\"]))) >> no_qe\n",
    "qe_wordnet = pt.apply.query(lambda q: strip_markup(expand_query_wordnet(q[\"query\"]))) >> no_qe\n",
    "qe_word2vec = pt.apply.query(lambda q: strip_markup(expand_query_word2vec(q[\"query\"]))) >> no_qe\n",
    "qe_pseudo = pt.apply.query(lambda q: strip_markup(expand_query_pseudo_relevance(doc_vecs, q[\"query\"], collection, vectorizer))) >> no_qe\n",
    "qe_stem = pt.apply.query(lambda q: strip_markup(expand_porter_stemmer(q[\"query\"]))) >> no_qe\n",
    "qe_comb = pt.apply.query(lambda q: strip_markup(comb(q[\"query\"], doc_vecs, collection, vectorizer))) >> no_qe\n",
    "pt.Experiment(\n",
    "    [no_qe, qe_stop, qe_sp, qe_sl, qe_stem, qe_wordnet, qe_word2vec, qe_pseudo, qe_comb],\n",
    "    dataset.get_topics()[:100],\n",
    "    dataset.get_qrels(),\n",
    "    eval_metrics=[pt.measures.MAP(rel=1), RR(rel=1), nDCG@10],\n",
    "    baseline = 0,\n",
    "    names = ['None', 'Stop', 'Stop-Porter', 'Stop-Lemma', 'Stemming', 'Wordnet', 'Word2Vec', 'Pseudo-relevance', 'Combined']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15d14b2a-4b02-42e1-8773-16fe362f198a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 08, 15:05:15] #> Loading collection...\n",
      "0M \n",
      "[Apr 08, 15:05:18] #> Loading codec...\n",
      "[Apr 08, 15:05:18] #> Loading IVF...\n",
      "[Apr 08, 15:05:18] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 881.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 08, 15:05:18] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . how much should i feed my 1 year old english mastiff, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2129,  2172,  2323,  1045,  5438,  2026,  1015,  2095,\n",
      "         2214,  2394, 15429, 13355,   102,   103,   103,   103,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103], device='cuda:0')\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>AP</th>\n",
       "      <th>RR</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>AP +</th>\n",
       "      <th>AP -</th>\n",
       "      <th>AP p-value</th>\n",
       "      <th>RR +</th>\n",
       "      <th>RR -</th>\n",
       "      <th>RR p-value</th>\n",
       "      <th>nDCG@10 +</th>\n",
       "      <th>nDCG@10 -</th>\n",
       "      <th>nDCG@10 p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>0.543767</td>\n",
       "      <td>0.734818</td>\n",
       "      <td>0.619591</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stop</td>\n",
       "      <td>0.513091</td>\n",
       "      <td>0.704623</td>\n",
       "      <td>0.585067</td>\n",
       "      <td>25.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>5.805481e-02</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.740075e-01</td>\n",
       "      <td>23.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.502095e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stop-Porter</td>\n",
       "      <td>0.475116</td>\n",
       "      <td>0.637293</td>\n",
       "      <td>0.537085</td>\n",
       "      <td>22.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.418708e-03</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.667636e-03</td>\n",
       "      <td>23.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.835455e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop-Lemma</td>\n",
       "      <td>0.520067</td>\n",
       "      <td>0.708850</td>\n",
       "      <td>0.588432</td>\n",
       "      <td>22.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.376274e-01</td>\n",
       "      <td>8.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.248369e-01</td>\n",
       "      <td>22.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.343160e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stemming</td>\n",
       "      <td>0.493347</td>\n",
       "      <td>0.649204</td>\n",
       "      <td>0.555943</td>\n",
       "      <td>21.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.360043e-03</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.612093e-03</td>\n",
       "      <td>16.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2.916572e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wordnet</td>\n",
       "      <td>0.399838</td>\n",
       "      <td>0.577746</td>\n",
       "      <td>0.469215</td>\n",
       "      <td>15.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.547434e-07</td>\n",
       "      <td>6.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.809109e-05</td>\n",
       "      <td>16.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2.662564e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0.440104</td>\n",
       "      <td>0.612290</td>\n",
       "      <td>0.502630</td>\n",
       "      <td>15.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>3.417246e-05</td>\n",
       "      <td>7.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.009065e-04</td>\n",
       "      <td>11.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>6.097241e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pseudo-relevance</td>\n",
       "      <td>0.510186</td>\n",
       "      <td>0.705128</td>\n",
       "      <td>0.571117</td>\n",
       "      <td>24.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5.145547e-03</td>\n",
       "      <td>11.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.836116e-02</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.290521e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Combined</td>\n",
       "      <td>0.250983</td>\n",
       "      <td>0.342083</td>\n",
       "      <td>0.289599</td>\n",
       "      <td>7.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>4.299057e-16</td>\n",
       "      <td>2.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2.699006e-15</td>\n",
       "      <td>5.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>5.117821e-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name        AP        RR   nDCG@10  AP +  AP -    AP p-value  \\\n",
       "0              None  0.543767  0.734818  0.619591   NaN   NaN           NaN   \n",
       "1              Stop  0.513091  0.704623  0.585067  25.0  42.0  5.805481e-02   \n",
       "2       Stop-Porter  0.475116  0.637293  0.537085  22.0  47.0  1.418708e-03   \n",
       "3        Stop-Lemma  0.520067  0.708850  0.588432  22.0  43.0  1.376274e-01   \n",
       "4          Stemming  0.493347  0.649204  0.555943  21.0  38.0  3.360043e-03   \n",
       "5           Wordnet  0.399838  0.577746  0.469215  15.0  64.0  1.547434e-07   \n",
       "6          Word2Vec  0.440104  0.612290  0.502630  15.0  59.0  3.417246e-05   \n",
       "7  Pseudo-relevance  0.510186  0.705128  0.571117  24.0  43.0  5.145547e-03   \n",
       "8          Combined  0.250983  0.342083  0.289599   7.0  79.0  4.299057e-16   \n",
       "\n",
       "   RR +  RR -    RR p-value  nDCG@10 +  nDCG@10 -  nDCG@10 p-value  \n",
       "0   NaN   NaN           NaN        NaN        NaN              NaN  \n",
       "1  10.0  22.0  1.740075e-01       23.0       33.0     2.502095e-02  \n",
       "2  10.0  30.0  6.667636e-03       23.0       40.0     1.835455e-04  \n",
       "3   8.0  22.0  2.248369e-01       22.0       32.0     4.343160e-02  \n",
       "4   7.0  25.0  4.612093e-03       16.0       32.0     2.916572e-04  \n",
       "5   6.0  41.0  1.809109e-05       16.0       57.0     2.662564e-08  \n",
       "6   7.0  38.0  1.009065e-04       11.0       55.0     6.097241e-07  \n",
       "7  11.0  22.0  8.836116e-02       19.0       36.0     1.290521e-04  \n",
       "8   2.0  68.0  2.699006e-15        5.0       74.0     5.117821e-18  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert import Searcher\n",
    "from colbert.data import Collection\n",
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "from pyterrier.measures import *\n",
    "if not pt.java.started():\n",
    "  pt.init()\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# data_dir = './project-root/lotte/raw/collection.tsv'\n",
    "# collection2 = Collection(data_dir)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    with Run().context(RunConfig(nranks=1, experiment=\"lotteindex\")):\n",
    "        config = ColBERTConfig(\n",
    "            root=\"experiments\",\n",
    "            # collection=collection2\n",
    "        )\n",
    "        searcher = Searcher(index=\"lotteindex.nbits_2\", config=config)\n",
    "\n",
    "class MyColbert:\n",
    "    def __init__(self, searcher, method):\n",
    "        self.searcher = searcher \n",
    "        self.method = method\n",
    "    \n",
    "    def transform(self, df):\n",
    "        results = None\n",
    "        for index, row in df.iterrows():\n",
    "            query = row['query']\n",
    "            if self.method is not None:\n",
    "                query = self.method(query)\n",
    "            result = self.searcher.search(query, k=100)\n",
    "            result = pd.DataFrame(result).transpose()\n",
    "            result = result.dropna()\n",
    "            result.columns = ['docno', 'rank', 'score']\n",
    "            result['qid'] = row['qid']\n",
    "            result['docno'] = result['docno'].astype(int)\n",
    "            if results is None:\n",
    "                results = result\n",
    "            else:\n",
    "                results = pd.concat([results, result])\n",
    "        return results\n",
    "\n",
    "\n",
    "no_qe = MyColbert(searcher, None)\n",
    "qe_stop = MyColbert(searcher, lambda x: stop_word(x))\n",
    "qe_sp = MyColbert(searcher, lambda x: stop_porter(x))\n",
    "qe_sl = MyColbert(searcher, lambda x: stop_lemma(x))\n",
    "qe_wordnet = MyColbert(searcher, lambda x: expand_query_wordnet(x))\n",
    "qe_word2vec = MyColbert(searcher, lambda x: expand_query_word2vec(x))\n",
    "qe_pseudo = MyColbert(searcher, lambda x: expand_query_pseudo_relevance(doc_vecs, x, collection, vectorizer))\n",
    "qe_stem = MyColbert(searcher, lambda x: expand_porter_stemmer(x))\n",
    "qe_comb = MyColbert(searcher, lambda x: comb(x, doc_vecs, collection, vectorizer))\n",
    "\n",
    "# print(dataset.get_qrels()[dataset.get_qrels()['qid'] == '3'])\n",
    "# print(searcher.search(dataset.get_topics()[3:5]['query'].to_list()[0], k=10))\n",
    "# bm25 = pt.terrier.Retriever(index, wmodel=\"BM25\", metadata=[\"docno\", \"text\"], properties={\"termpipelines\": \"\"}, controls={\"qe\": \"off\"})\n",
    "# print(bm25.search(dataset.get_topics()[3:4]['query'].tolist()[0]))\n",
    "pt.Experiment(\n",
    "    [no_qe, qe_stop, qe_sp, qe_sl, qe_stem, qe_wordnet, qe_word2vec, qe_pseudo, qe_comb],\n",
    "    # [bm25, no_qe, qe_stop],\n",
    "    dataset.get_topics()[:100],\n",
    "    dataset.get_qrels(),\n",
    "    eval_metrics=[pt.measures.MAP(rel=1), RR(rel=1), nDCG@10],\n",
    "    baseline = 0,\n",
    "    names = ['None', 'Stop', 'Stop-Porter', 'Stop-Lemma', 'Stemming', 'Wordnet', 'Word2Vec', 'Pseudo-relevance', 'Combined']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f37efbb-364c-4051-900a-fe4ae7a24549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:02:16.934 [main] WARN org.terrier.querying.ApplyTermPipeline -- The index has no termpipelines configuration, and no control configuration is found. Defaulting to global termpipelines configuration of ''. Set a termpipelines control to remove this warning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>AP</th>\n",
       "      <th>RR</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>AP +</th>\n",
       "      <th>AP -</th>\n",
       "      <th>AP p-value</th>\n",
       "      <th>RR +</th>\n",
       "      <th>RR -</th>\n",
       "      <th>RR p-value</th>\n",
       "      <th>nDCG@10 +</th>\n",
       "      <th>nDCG@10 -</th>\n",
       "      <th>nDCG@10 p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>0.059785</td>\n",
       "      <td>0.103185</td>\n",
       "      <td>0.082706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stop</td>\n",
       "      <td>0.058037</td>\n",
       "      <td>0.098661</td>\n",
       "      <td>0.077050</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.472541</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.348213</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.347125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stop-Porter</td>\n",
       "      <td>0.131126</td>\n",
       "      <td>0.215826</td>\n",
       "      <td>0.181941</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.015466</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.030976</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.006221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop-Lemma</td>\n",
       "      <td>0.077329</td>\n",
       "      <td>0.142662</td>\n",
       "      <td>0.103562</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.239002</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.168471</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.370761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stemming</td>\n",
       "      <td>0.128499</td>\n",
       "      <td>0.209785</td>\n",
       "      <td>0.183004</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.014539</td>\n",
       "      <td>24.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.024283</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.003392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wordnet</td>\n",
       "      <td>0.047081</td>\n",
       "      <td>0.094073</td>\n",
       "      <td>0.065288</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.381543</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.815112</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.349704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0.045462</td>\n",
       "      <td>0.080509</td>\n",
       "      <td>0.068017</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.334337</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.456105</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.410031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pseudo-relevance</td>\n",
       "      <td>0.043942</td>\n",
       "      <td>0.078486</td>\n",
       "      <td>0.062751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.085775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.068890</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.184552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Combined</td>\n",
       "      <td>0.059283</td>\n",
       "      <td>0.101240</td>\n",
       "      <td>0.085180</td>\n",
       "      <td>22.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.976811</td>\n",
       "      <td>21.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.962632</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.923149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name        AP        RR   nDCG@10  AP +  AP -  AP p-value  \\\n",
       "0              None  0.059785  0.103185  0.082706   NaN   NaN         NaN   \n",
       "1              Stop  0.058037  0.098661  0.077050  17.0   2.0    0.472541   \n",
       "2       Stop-Porter  0.131126  0.215826  0.181941  27.0   9.0    0.015466   \n",
       "3        Stop-Lemma  0.077329  0.142662  0.103562  23.0   3.0    0.239002   \n",
       "4          Stemming  0.128499  0.209785  0.183004  25.0   7.0    0.014539   \n",
       "5           Wordnet  0.047081  0.094073  0.065288  13.0  18.0    0.381543   \n",
       "6          Word2Vec  0.045462  0.080509  0.068017  14.0  17.0    0.334337   \n",
       "7  Pseudo-relevance  0.043942  0.078486  0.062751   0.0  24.0    0.085775   \n",
       "8          Combined  0.059283  0.101240  0.085180  22.0  15.0    0.976811   \n",
       "\n",
       "   RR +  RR -  RR p-value  nDCG@10 +  nDCG@10 -  nDCG@10 p-value  \n",
       "0   NaN   NaN         NaN        NaN        NaN              NaN  \n",
       "1  13.0   3.0    0.348213        1.0        1.0         0.347125  \n",
       "2  26.0   8.0    0.030976       14.0        3.0         0.006221  \n",
       "3  20.0   4.0    0.168471        4.0        2.0         0.370761  \n",
       "4  24.0   7.0    0.024283       14.0        2.0         0.003392  \n",
       "5  13.0  16.0    0.815112        4.0        6.0         0.349704  \n",
       "6  14.0  16.0    0.456105        5.0        5.0         0.410031  \n",
       "7   0.0  22.0    0.068890        0.0        4.0         0.184552  \n",
       "8  21.0  15.0    0.962632        7.0        6.0         0.923149  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from pyterrier.measures import *\n",
    "params = {'objective': 'rank:ndcg',\n",
    "          'learning_rate': 0.1,\n",
    "          'gamma': 1.0, \n",
    "          'min_child_weight': 0.1,\n",
    "          'max_depth': 6,\n",
    "          'random_state': 42\n",
    "         }\n",
    "topics = dataset.get_topics()[:200]\n",
    "qrels = dataset.get_qrels()\n",
    "train_topics, valid_topics, test_topics = np.split(topics, [int(.6*len(topics)), int(.8*len(topics))])\n",
    "\n",
    "fbr3f = pt.terrier.FeaturesRetriever(index, wmodel=\"BM25\", features=  ['WMODEL:TF_IDF', 'WMODEL:PL2', 'WMODEL:BM25', \n",
    "                                                 'WMODEL:DirichletLM', 'WMODEL:Hiemstra_LM', \n",
    "                                                 'WMODEL:DFR_BM25', 'WMODEL:InL2', 'WMODEL:LGD', \n",
    "                                                 'WMODEL:DLH', 'WMODEL:DPH', 'WMODEL:LemurTF_IDF'], properties={\"termpipelines\": \"\"}, controls={\"qe\": \"off\"})\n",
    "BaseLTR_LM = fbr3f >> pt.ltr.apply_learned_model(xgb.sklearn.XGBRanker(**params), form='ltr')\n",
    "BaseLTR_LM.fit(train_topics, qrels, valid_topics, qrels)\n",
    "\n",
    "no_qe = BaseLTR_LM\n",
    "qe_stop = pt.apply.query(lambda q: strip_markup(stop_word(q[\"query\"]))) >> no_qe\n",
    "qe_sp = pt.apply.query(lambda q: strip_markup(stop_porter(q[\"query\"]))) >> no_qe\n",
    "qe_sl = pt.apply.query(lambda q: strip_markup(stop_lemma(q[\"query\"]))) >> no_qe\n",
    "qe_stem = pt.apply.query(lambda q: strip_markup(expand_porter_stemmer(q[\"query\"]))) >> no_qe\n",
    "qe_wordnet = pt.apply.query(lambda q: strip_markup(expand_query_wordnet(q[\"query\"]))) >> no_qe\n",
    "qe_word2vec = pt.apply.query(lambda q: strip_markup(expand_query_word2vec(q[\"query\"]))) >> no_qe\n",
    "qe_pseudo = pt.apply.query(lambda q: strip_markup(expand_query_pseudo_relevance(doc_vecs, q[\"query\"], collection, vectorizer))) >> no_qe\n",
    "qe_comb = pt.apply.query(lambda q: strip_markup(comb(q[\"query\"], doc_vecs, collection, vectorizer))) >> no_qe\n",
    "\n",
    "\n",
    "pt.Experiment(\n",
    "    [no_qe, qe_stop, qe_sp, qe_sl, qe_stem, qe_wordnet, qe_word2vec, qe_pseudo, qe_comb],\n",
    "    test_topics,\n",
    "    qrels,\n",
    "    eval_metrics=[pt.measures.MAP(rel=1), RR(rel=1), nDCG@10],\n",
    "    baseline = 0,\n",
    "    names = ['None', 'Stop', 'Stop-Porter', 'Stop-Lemma', 'Stemming', 'Wordnet', 'Word2Vec', 'Pseudo-relevance', 'Combined']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb9d98-2554-4b9d-8cb3-d77d2077a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "from pyterrier.measures import *\n",
    "from pyterrier_t5 import MonoT5ReRanker, DuoT5ReRanker\n",
    "monoT5 = MonoT5ReRanker() \n",
    "duoT5 = DuoT5ReRanker() \n",
    "\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\", properties={\"termpipelines\": \"\"}, controls={\"qe\": \"off\"})\n",
    "mono_pipeline = (bm25 % 50) >> pt.text.get_text(dataset, \"text\") >> monoT5\n",
    "duo_pipeline = mono_pipeline % 5 >> duoT5\n",
    "\n",
    "no_qe = duo_pipeline\n",
    "qe_stop = pt.apply.query(lambda q: strip_markup(stop_word(q[\"query\"]))) >> no_qe\n",
    "qe_sp = pt.apply.query(lambda q: strip_markup(stop_porter(q[\"query\"]))) >> no_qe\n",
    "qe_sl = pt.apply.query(lambda q: strip_markup(stop_lemma(q[\"query\"]))) >> no_qe\n",
    "qe_stem = pt.apply.query(lambda q: strip_markup(expand_porter_stemmer(q[\"query\"]))) >> no_qe\n",
    "qe_wordnet = pt.apply.query(lambda q: strip_markup(expand_query_wordnet(q[\"query\"]))) >> no_qe\n",
    "qe_word2vec = pt.apply.query(lambda q: strip_markup(expand_query_word2vec(q[\"query\"]))) >> no_qe\n",
    "qe_pseudo = pt.apply.query(lambda q: strip_markup(expand_query_pseudo_relevance(doc_vecs, q[\"query\"], collection, vectorizer))) >> no_qe\n",
    "qe_comb = pt.apply.query(lambda q: strip_markup(comb(q[\"query\"], doc_vecs, collection, vectorizer))) >> no_qe\n",
    "\n",
    "\n",
    "pt.Experiment(\n",
    "    [no_qe, qe_stop, qe_sp, qe_sl, qe_stem, qe_wordnet, qe_word2vec, qe_pseudo, qe_comb],\n",
    "    dataset.get_topics()[:50],\n",
    "    dataset.get_qrels(),\n",
    "    eval_metrics=[pt.measures.MAP(rel=1), RR(rel=1), nDCG@10],\n",
    "    baseline = 0,\n",
    "    names = ['None', 'Stop', 'Stop-Porter', 'Stop-Lemma', 'Stemming', 'Wordnet', 'Word2Vec', 'Pseudo-relevance', 'Combined']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f76796-c6c5-410f-9f35-adc006c91e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06117ee4-479d-4495-9dd8-60e25610ff8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
